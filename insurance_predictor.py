# -*- coding: utf-8 -*-
"""MLProject

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MZqkpxK4V4jNTBZv08juyUy881FuvhmJ
"""

#Imports
import pandas as pd
import numpy as np
import seaborn as sns
#For the heatmap
import matplotlib.pyplot as plt
#Plotly for making histograms
import plotly.graph_objs as go
import plotly.offline as py
#For building a neural network
from keras.models import Sequential
from keras.layers import Dense
from sklearn.model_selection import train_test_split
from keras.layers.advanced_activations import LeakyReLU
from keras.layers import Dropout

#For checking how lone it takes to run each NN
import time
#For plotting loss and validation losss over time
import matplotlib.pyplot as plt
#Final check for sccuracy and roc
from sklearn.metrics import accuracy_score
from sklearn.metrics import roc_auc_score
from sklearn import metrics

#Load the csvs from my drive starting here
from google.colab import drive
drive.mount('/content/drive')

#Loads the csvs into train and test dfs
#I only load the training data since I can't get access to testing target values
print("Loading the data")
loc = "/content/drive/MyDrive/ML_Project/train.csv"
df_train = pd.read_csv(loc)
#loc2 = "/content/drive/MyDrive/ML_Project/test.csv"
#df_test = pd.read_csv(loc2)

#Prints the columns present in both dfs (testing is missing the target though)
print(df_train.columns)

#How many rows do we end up with if we drop all na?
df_clean_train=df_train.replace(-1,np.nan).dropna(axis = 0, how = 'any')
df_clean_train

#Create a df replacing any nan with -1 for a heatmap
df_clean_test=df_test.replace(-1,np.nan).dropna(axis = 0, how = 'any')
df_clean_test

#Create a heatmap
plt.figure(figsize=(16,16))
sns.heatmap(df_clean_train.corr())

#How many filers vs nonfilers do we have?
ax = sns.countplot(x="target", data=df_train)

#Here we create a barplot that shows us the frequency of binary values for all binary cols
#From here, I decided to drop ps_ind_[10-13]_bin because they all have no correlations with the target and are almost all 0s only
bin_col = [col for col in df_train.columns if '_bin' in col]
zero_list = []
one_list = []
for col in bin_col:
    zero_list.append((df_train[col]==0).sum())
    one_list.append((df_train[col]==1).sum())

trace1 = go.Bar(
    x=bin_col,
    y=zero_list ,
    name='0'
)
trace2 = go.Bar(
    x=bin_col,
    y=one_list,
    name='1'
)

data = [trace1, trace2]
layout = go.Layout(
    barmode='stack',
    title='What does the distribution of binary values look like for each col?'
)

fig = go.Figure(data=data, layout=layout)
py.iplot(fig, filename='stacked-bar')

#Drop cols with many missing values and id
drop_cols = ['id', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin']
df_remove = df_train.drop(drop_cols, axis =1 )
df_clean_test=df_remove.replace(-1,np.nan).dropna(axis = 0, how = 'any')
df_remove = df_train.drop(drop_cols, axis =1 )

#Let's see how standardize vs normalzation works
from sklearn import preprocessing
cols = df_clean_test.columns
min_max_scaler = preprocessing.MinMaxScaler()
x_scaled = min_max_scaler.fit_transform(df_clean_test)
newdf_norm = pd.DataFrame(x_scaled, columns = cols)
print(newdf_norm)

#Start the sampling here
nofiled_counts, filed_counts = df_train.target.value_counts()
df_nofile = df_train[df_train['target'] == 0]
df_file = df_train[df_train['target'] == 1]
plt.style.use('seaborn')
print(df_nofile)

#This cell under-samples the data, grabbing all rows where somewhere filed (5%) + and equal number of unfiled rows
#Partially inspired by user Kaggle user Rafael's discussion post though they don't explain which of these two cells works better
temp_nofile = df_nofile.sample(filed_counts)
undersample_df = pd.concat([temp_nofile, df_file], axis=0)
undersample_df.target.value_counts().plot(kind='bar', title='Sum of Rows When Undersampling', color=['red', 'blue'])

#Conversely this cell over-samples, duplicating filed rows to match the total amount of unfilled rows
temp_file = df_file.sample(nofiled_counts, replace=True)
oversample_df = pd.concat([df_nofile, temp_file], axis=0)
oversample_df.target.value_counts().plot(kind='bar', title='Sum of Rows When Oversampling', color=['red', 'blue'])

#Create dfs to try oversampling or undersampling
#Drop cols with many missing values and id
#Undersample
drop_cols = ['id', 'ps_ind_10_bin', 'ps_ind_11_bin', 'ps_ind_12_bin', 'ps_ind_13_bin']
x = undersample_df
x = x.drop(drop_cols, axis =1 )
y = undersample_df['target']

#Oversample
x2 = oversample_df
x2 = x2.drop(drop_cols, axis =1 )
y2 = oversample_df['target']

X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42, stratify=y)
X_train2, X_test2, y_train2, y_test2 = train_test_split(x2, y2, test_size=0.2, random_state=42, stratify=y2)

#Set contstants for both models
vl = 0.2
epoch = 100
bs = 32
dr = 0.2
alpha_val = 0.3

#Under
model = Sequential()
model.add(Dense(100, input_dim=len(x.columns), activation='relu'))
model.add(Dense(8, activation=LeakyReLU(alpha=alpha_val)))
model.add(Dropout(dr))
model.add(Dense(8, activation=LeakyReLU(alpha=alpha_val)))
model.add(Dropout(dr))
model.add(Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model2 = model

print(X_test)
print(y_test)

start = time.time()
history = model.fit(X_train, y_train, validation_split=vl, epochs=epoch, batch_size=bs)
end = time.time()
print("Time to run undersample model:")
print(end - start)

start = time.time()
history2 = model2.fit(X_train2, y_train2, validation_split=vl, epochs=epoch, batch_size=bs)
end = time.time()
print("Time to run undersample model:")
print(end - start)

#Let's compare sampling techniques
_, accuracy = model.evaluate(X_test, y_test)
print('Accuracy of undersampling: %.2f' % (accuracy*100))
_, accuracy2 = model2.evaluate(X_test2, y_test2)
print('Accuracy of oversampling: %.2f' % (accuracy2*100))

#Plot model for undersampling

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

#Plot model for oversampling
plt.plot(history2.history['accuracy'])
plt.plot(history2.history['val_accuracy'])
plt.legend(['train', 'test'], loc='upper left')
plt.show()

plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

from sklearn.metrics import roc_auc_score
y_pred = (model.predict(X_test) > 0.5).astype("int32")
print("Accuracy is " , accuracy_score(y_test, y_pred))
print("ROC is " , roc_auc_score(y_test, y_pred))

y_pred2 = (model2.predict(X_test2) > 0.5).astype("int32")
accuracy_score(y_test2, y_pred2)

while True:pass